<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on Alex Kyllo</title>
		<link>https://alexkyllo.com/posts/</link>
		<description>Recent content in Posts on Alex Kyllo</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>Alex Kyllo 2020</copyright>
		<lastBuildDate>Mon, 24 Aug 2020 21:56:31 -0700</lastBuildDate>
		<atom:link href="https://alexkyllo.com/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Easy Python Package Publishing with Poetry</title>
			<link>https://alexkyllo.com/posts/python-package/</link>
			<pubDate>Mon, 24 Aug 2020 21:56:31 -0700</pubDate>
			
			<guid>https://alexkyllo.com/posts/python-package/</guid>
			<description>I just published my first Python package on PyPI, called feature-grouper, a data science package for a simple form of dimensionality reduction.
The package itself is almost trivial, a couple of functions and a scikit-learn transformer class, but it&amp;rsquo;s something I anticipate reusing on future projects, so I wanted to be able to just import it rather than copy-pasting the code.
I thought I&amp;rsquo;d write a post to detail the end-to-end development process to remind myself the next time I want to do it, and in case anyone else finds this useful.</description>
			<content type="html"><![CDATA[<p>I just published my first Python package on PyPI, called
<a href="https://pypi.org/project/feature-grouper/">feature-grouper</a>,
a data science package for a simple form of dimensionality reduction.</p>
<p>The package itself is almost trivial, a couple of functions and
a scikit-learn transformer class, but it&rsquo;s something I anticipate
reusing on future projects, so I wanted to be able to just import it
rather than copy-pasting the code.</p>
<p>I thought I&rsquo;d write a post to detail the end-to-end development
process to remind myself the next time I want to do it, and in
case anyone else finds this useful. I used
<a href="https://python-poetry.org">Poetry</a> for package management and while
the process was pretty smooth and straightforward, there were a couple
of gotchas that deserve explanation.</p>
<h2 id="installing-the-tools">Installing the tools</h2>
<p>Installing Python (on Ubuntu):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.8 python3-pip python3.8-dev python3.8-venv make
python3.8 -m pip install --upgrade pip setuptools wheel
</code></pre></div><p>Installing Poetry:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python3
</code></pre></div><h2 id="initializing-the-project">Initializing the project</h2>
<p>I used the <code>poetry new</code> command to create the project structure.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">poetry new feature-grouper
</code></pre></div><p>Poetry includes pytest by default (a good choice) and gives you a
nice, simple package project directory like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">cd feature-grouper
tree

.
├── feature_grouper
│   ├── feature_grouper.py
│   ├── __init__.py
├── pyproject.toml
├── README.rst
└── tests
    ├── __init__.py
    └── test_feature_grouper.py

</code></pre></div><p>The <code>pyproject.toml</code> file replaces <code>setup.py</code> for Poetry projects, so
I opened that file and updated the package description field.</p>
<p><code>poetry install</code> auto-created a virtual environment the first time,
and installed the base dependencies, generating a <code>poetry.lock</code> file
that records the entire package dependency tree with the compatible version
numbers.</p>
<p><code>poetry shell</code> activates the virtual environment and <code>exit</code> deactivates it.</p>
<h2 id="installing-dependencies">Installing dependencies</h2>
<p>The next step was to add and install the dependencies I knew I would need to
develop the package.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">poetry add scipy numpy scikit-learn
poetry add --dev black pylint wrapt Sphinx sphinx-rtd-theme
</code></pre></div><p>The package itself depends on <code>scipy</code>, <code>numpy</code>
and <code>scikit-learn</code>, meanwhile I use <code>black</code> and <code>pylint</code> for code
formatting and linting, and Sphinx for documentation, so I install
those as dev dependencies.</p>
<p><code>poetry add</code> adds the dependencies to <code>pyproject.toml</code>, solves
the dependency graph, updatess <code>poetry.lock</code> and installs the
packages into the virtual environemnt, all in one step.</p>
<p><code>poetry export --without-hashes &gt; requirements.txt</code> generates a
<code>requirements.txt</code> file, which can be used by other tools that need to
install the dependencies.</p>
<h2 id="testing">Testing</h2>
<p>Getting started with my test suite was a snap. I just had to open
<code>tests/test_feature_grouper.py</code> and start writing functions with
assert statements, and run them with <code>pytest</code>. That made it easy to
get started with test-driven development.</p>
<h2 id="coding">Coding</h2>
<p>The <code>feature_grouper/feature_grouper.py</code> file is only 131 lines including
comments, so there isn&rsquo;t a whole lot going on&ndash;the key point is that the class
<code>FeatureGrouper</code> extends <code>BaseEstimator</code> and <code>TransformerMixin</code> from the
<code>sklearn.base</code> module, and implements its own <code>fit</code>, <code>transform</code>, and
<code>inverse_transform</code> methods, to make it basically a drop-in replacement for
an existing scikit-learn transformer class like
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn.decomposition.PCA</a>, so that it can be used in a
<a href="https://scikit-learn.org/stable/modules/classes.html?highlight=pipeline#module-sklearn.pipeline">sklearn.pipeline.Pipeline</a>.</p>
<p>I also want to mention that I find it better to write the class and function
docstrings as I go, rather than writing all the code first and going
back to document it later. The latter always feels like way more work.
I went with
<a href="https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html">Sphinx-style docstrings</a>
but for my next package I will probably try
<a href="https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_numpy.html">Numpy-style docstrings</a>
as they are a little more readable in plaintext and seem more popular in the
Python community.</p>
<h2 id="documentation">Documentation</h2>
<p>Sphinx is the go-to for generating code documentation for Python
packages. It includes a <code>sphinx-quickstart</code> CLI to help you get
started.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">mkdir docs
cd docs
sphinx-quickstart
</code></pre></div><p>Then, so that Sphinx could find my code to autogenerate documentation pages
from the docstrings I had to add these lines to conf.py:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># -- Path setup --------------------------------------------------------------</span>

<span style="color:#75715e"># If extensions (or modules to document with autodoc) are in another directory,</span>
<span style="color:#75715e"># add these directories to sys.path here. If the directory is relative to the</span>
<span style="color:#75715e"># documentation root, use os.path.abspath to make it absolute, like shown here.</span>
<span style="color:#75715e">#</span>
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> sys

sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>insert(<span style="color:#ae81ff">0</span>, os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>abspath(<span style="color:#e6db74">&#34;..&#34;</span>))
</code></pre></div><p>I like the Read The Docs theme for documentation so I wanted to include that theme
as well as autodoc to  generate the documentation from my class and function docstrings,
so I added the <code>sphinx.ext.autodoc</code> and <code>sphinx_rtd_theme</code> extensions in <code>conf.py</code>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># -- General configuration ---------------------------------------------------</span>

<span style="color:#75715e"># Add any Sphinx extension module names here, as strings. They can be</span>
<span style="color:#75715e"># extensions coming with Sphinx (named &#39;sphinx.ext.*&#39;) or your custom</span>
<span style="color:#75715e"># ones.</span>
extensions <span style="color:#f92672">=</span> [
    <span style="color:#e6db74">&#34;sphinx.ext.todo&#34;</span>,
    <span style="color:#e6db74">&#34;sphinx.ext.viewcode&#34;</span>,
    <span style="color:#e6db74">&#34;sphinx.ext.autodoc&#34;</span>,
    <span style="color:#e6db74">&#34;sphinx_rtd_theme&#34;</span>,
]

</code></pre></div><p>Then, to provide the content pages for the docs site,
I added two files, <code>docs/overview.rst</code> and <code>docs/reference.rst</code>. In the
overview file I put a description of the package and a code sample, and in the
reference file I put the following:</p>
<pre><code>``feature_grouper`` API reference
=================================

.. automodule:: feature_grouper.feature_grouper
   :members:

</code></pre><p>The <code>automodule</code> thing tells Sphinx to read all the class and function docstrings
in your Python code file, and generate API documentation for them.</p>
<p>Sphinx includes a Makefile so you can build the docs site by just typing <code>make html</code>
and you can preview the HTML output in your browser. Each time you change a docstring,
just do <code>make html</code> again and it will rebuild the docs.</p>
<p>After pushing my package to GitHub,
I published my documentation to <a href="https://readthedocs.org/">Read The Docs</a>
because they have free hosting that is pretty easy to configure.
RTD failed to build my docs the first time, because it was
looking for a file called &ldquo;content&rdquo; by default, but I had named my file
&ldquo;index&rdquo;&ndash;had to make a trip to StackOverflow
for that one. It turned out I just needed to add
another setting to docs/conf.py, <code>master_doc = &quot;index&quot;</code> in order to
get it to find the right file. At that point, the documentation was done.</p>
<h2 id="publishing-to-pypi">Publishing to PyPI</h2>
<p>Creating an account on <a href="https://pypi.org/">PyPI</a> is pretty straightforward, you
just sign up through their web interface. I turned Two-Factor Authentication on
and added an API token in Account settings.</p>
<p>One configuration was needed to set up Poetry to publish my package, using the
API token I obtained from the PyPI site:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">poetry config pypi-token.pypi <span style="color:#f92672">[</span>api-token<span style="color:#f92672">]</span>
</code></pre></div><p>Once that was set, after typing <code>poetry publish</code> my package was live on PyPI
within seconds!</p>
]]></content>
		</item>
		
		<item>
			<title>Database Learning Resources</title>
			<link>https://alexkyllo.com/posts/database-learning/</link>
			<pubDate>Sat, 09 May 2020 11:04:29 -0700</pubDate>
			
			<guid>https://alexkyllo.com/posts/database-learning/</guid>
			<description>Over the last few years working in analytics and data science, I&amp;rsquo;ve developed a strong interest in the internal workings of database engines, particularly distributed database for analytical workloads. Here I&amp;rsquo;m going to build a list of computer science concepts and learning resources that I am finding helpful in building understanding of how these systems work and how to implement them.
File I/O Concepts  On Disk I/O a series of blog posts by Alex Petrov, explaining the different methods of file I/O in Linux (buffered, direct, memory-mapped, asynchronous, vectored) and their use cases in the context of database applications, as well as on-disk data structures (SSTables, B+ Trees, Log-Structured Merge Trees)  Probabilistic Data Structures  Bloom Filter A hash bitmap for probabilistic answers to set membership queries (true/false with no false negatives) HyperLogLog A hash based algorithm for fast approximate cardinality (distinct count) estimation Count Min Sketch an approximate frequency table similar to a counting Bloom filter  On-Disk Data Structures  LSM Tree Paper Log Structured Merge Trees&amp;ndash;trees of immutable sorted runs of on-disk data that are periodically merged using external merge sort Google BigTable Paper Application of LSM Trees  Distributed Consensus Algorithms  The Raft Consensus Algorithm Implementing Raft by Eli Bendersky  Comprehensive Database Concept Books  Database System Concepts Database Internals Designing Data Intensive Applications  Open Courses  CMU 15-445 Database Systems covers basics of on-disk database system implementation CMU 15-721 Advanced Database Systems covers advanced topics in in-memory database implementation, with emphasis on concurrency control and optimizations.</description>
			<content type="html"><![CDATA[<p>Over the last few years working in analytics and data science, I&rsquo;ve developed a
strong interest in the internal workings of database engines, particularly
distributed database for analytical workloads. Here I&rsquo;m going to build a list of
computer science concepts and learning resources that I am finding helpful in
building understanding of how these systems work and how to implement them.</p>
<h1 id="file-io-concepts">File I/O Concepts</h1>
<ul>
<li><a href="https://medium.com/databasss/on-disk-io-part-1-flavours-of-io-8e1ace1de017">On Disk I/O</a>
a series of blog posts by Alex Petrov, explaining the different methods of
file I/O in Linux (buffered, direct, memory-mapped, asynchronous, vectored)
and their use cases in the context of database applications, as well as on-disk
data structures (SSTables, B+ Trees, Log-Structured Merge Trees)</li>
</ul>
<h2 id="probabilistic-data-structures">Probabilistic Data Structures</h2>
<ul>
<li><a href="https://www.jasondavies.com/bloomfilter/">Bloom Filter</a> A hash bitmap for
probabilistic answers to set membership queries (true/false with no false negatives)</li>
<li><a href="http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf">HyperLogLog</a> A hash based
algorithm for fast approximate cardinality (distinct count) estimation</li>
<li><a href="https://redislabs.com/blog/count-min-sketch-the-art-and-science-of-estimating-stuff/">Count Min Sketch</a>
an approximate frequency table similar to a counting Bloom filter</li>
</ul>
<h1 id="on-disk-data-structures">On-Disk Data Structures</h1>
<ul>
<li><a href="http://paperhub.s3.amazonaws.com/18e91eb4db2114a06ea614f0384f2784.pdf">LSM Tree Paper</a>
Log Structured Merge Trees&ndash;trees of immutable sorted runs of on-disk data that are periodically
merged using external merge sort</li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Google BigTable Paper</a> Application of LSM Trees</li>
</ul>
<h1 id="distributed-consensus-algorithms">Distributed Consensus Algorithms</h1>
<ul>
<li><a href="https://raft.github.io/">The Raft Consensus Algorithm</a></li>
<li><a href="https://eli.thegreenplace.net/2020/implementing-raft-part-0-introduction/">Implementing Raft by Eli Bendersky</a></li>
</ul>
<h1 id="comprehensive-database-concept-books">Comprehensive Database Concept Books</h1>
<ul>
<li><a href="https://www.db-book.com/db7/index.html">Database System Concepts</a></li>
<li><a href="https://learning.oreilly.com/library/view/database-internals/9781492040330/">Database Internals</a></li>
<li><a href="https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/">Designing Data Intensive Applications</a></li>
</ul>
<h1 id="open-courses">Open Courses</h1>
<ul>
<li><a href="https://15445.courses.cs.cmu.edu/fall2019/">CMU 15-445 Database Systems</a>
covers basics of on-disk database system implementation</li>
<li><a href="https://15721.courses.cs.cmu.edu/spring2020/">CMU 15-721 Advanced Database Systems</a>
covers advanced topics in in-memory database implementation, with emphasis on concurrency
control and optimizations.</li>
</ul>
]]></content>
		</item>
		
		<item>
			<title>Make Is All You Need</title>
			<link>https://alexkyllo.com/posts/make-is-all-you-need/</link>
			<pubDate>Mon, 30 Dec 2019 18:03:52 -0800</pubDate>
			
			<guid>https://alexkyllo.com/posts/make-is-all-you-need/</guid>
			<description>As a data analyst mostly working with high level languages and GUI tools, I thought of GNU Make as a tool (from 1976!) for building Linux binaries from compiled languages, and it didn&amp;rsquo;t occur to me how it could be useful in my workflow. Make is usually part of any curriculum for software engineers learning C/C++, but I seldom if ever see it mentioned in data science courses or tutorials.</description>
			<content type="html"><![CDATA[<p>As a data analyst mostly working with high level languages and GUI tools,
I thought of <a href="https://www.gnu.org/software/make/">GNU Make</a> as a tool
(from 1976!) for building Linux binaries from compiled languages,
and it didn&rsquo;t occur to me how it could be useful in my workflow. Make is
usually part of any curriculum for software engineers learning C/C++, but
I seldom if ever see it mentioned in data science courses or tutorials.</p>
<p>Today, I use Make for all of my software and data science projects, and I
consider it the most important tool for reproducibility. It ships with
Linux and Mac, and there is a <a href="http://gnuwin32.sourceforge.net/packages/make.htm">Windows installer</a>
available. I love that it&rsquo;s language-agnostic, so it&rsquo;s perfect for teams
where different people use R, Python, or other languages for their analysis code.
It also gives you a &ldquo;single point of contact&rdquo; for all the scripts and commands
in your project, so you can just type <code>make</code> or
<code>make [task name]</code> instead of having to remember and re-type each command.</p>
<p>The <a href="https://www.gnu.org/software/make/manual/make.html">GNU Make Manual</a>
is quite approachable while also providing great detail, but in a nutshell,
you create a file called <code>Makefile</code> in your project directory and add to it
a set of &ldquo;rules&rdquo; for producing output files from input files, in this form:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-make" data-lang="make"><span style="color:#a6e22e">targets </span><span style="color:#f92672">:</span> prerequisites
    recipe
</code></pre></div><p>A &ldquo;target&rdquo; and a &ldquo;prerequisite&rdquo; are usually filenames, but can also be just
names for tasks (called &ldquo;phony&rdquo; targets).</p>
<p>So why is this so useful for data science projects?</p>
<p>Data analysis code typically involves five steps for processing data:</p>
<ol>
<li>Getting it</li>
<li>Cleaning it</li>
<li>Exploring it</li>
<li>Modeling it</li>
<li>Interpreting it</li>
</ol>
<p>Without a disciplined approach, a typical R or Python script or notebook file
will do all of these steps together, perhaps interleaved, making it difficult
to pull the steps apart so that they can be executed separately.</p>
<p>Why is it important to separate them? Some steps may process large quantities of
data, so you don&rsquo;t want to re-run everything each time you change your script.
Caching intermediate results saves you a lot of time as you&rsquo;re developing.</p>
<p>Also, if you have mutli-language scenarios, say you want to visualize your data
with <a href="https://ggplot2.tidyverse.org/">ggplot2</a>
before training a predictive model with <a href="https://keras.io/">Keras</a>,
it&rsquo;s easiest to separate these steps into
different scripts in different languages, rather than doing something
complicated like spawning a Python subprocess from within R or vice versa.</p>
<p>Here&rsquo;s an example of what a Makefile might look like, for generating a paper
written in LaTeX, containing two plots generated by an R script and a Python
script, from a CSV file created by a SQL database query:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-make" data-lang="make"><span style="color:#a6e22e">default </span><span style="color:#f92672">:</span> paper.pdf

<span style="color:#a6e22e">paper.pdf </span><span style="color:#f92672">:</span> paper.tex figure-01.png figure-02.png
    pdflatex paper.tex

<span style="color:#a6e22e">figure-01.png </span><span style="color:#f92672">:</span> plot.R data.csv
    Rscript plot.R

<span style="color:#a6e22e">figure-02.png </span><span style="color:#f92672">:</span> plot.py data.csv
    python plot.py

<span style="color:#a6e22e">data.csv</span><span style="color:#f92672">:</span> query.sql
    sql2csv --db <span style="color:#e6db74">&#34;sqlite:///database.db&#34;</span> query.sql
</code></pre></div><p>Given this Makefile, when you type <code>make</code>, it will execute each of these
recipes in dependency order, and then if you type <code>make</code> again, it will
do nothing, because the dependencies haven&rsquo;t changed. Make looks at the
file modified timestamps to see which steps actually need to be redone.
If you change the <code>query.sql</code> file and type <code>make</code> again, it will rerun
everything because that&rsquo;s the first item in the dependency chain, but if you
only change the <code>paper.tex</code> file, then only the
recipe for <code>paper.pdf</code> will be rerun the next time you type <code>make</code>, because
nothing upstream of it changed. This is an incredible time-saver when some of
your recipes take a long time to run, as is often the case with data science projects.
I&rsquo;ve seen many other tools designed to solve this problem, most of them
language-specific and much more complex to learn and use than Make.</p>
<p>As far as how to structure a new project, there isn&rsquo;t really an accepted standard
data science code project structure or framework.
The closest thing might be
<a href="https://drivendata.github.io/cookiecutter-data-science/#directory-structure">Cookiecutter Data Science</a>
(which also utilizes a Makefile), though it&rsquo;s Python-specific and I haven&rsquo;t seen
it used &ldquo;in the wild&rdquo; yet. If you use a Makefile in your project, though, it doesn&rsquo;t
really matter that much how you structure your directories or even what language you use,
as thinking in terms of Make recipes will encourage you to break up your analysis
scripts into a pipeline of small,
dependent, repeatable steps with cached results. Then when a colleague picks up
your project to review your work or enhance it, just looking at the Makefile will
give them a clear idea of how it&rsquo;s put together.
So use Make for your data science projects and encourage your coworkers to do the
same&ndash;you will thank each other!</p>
]]></content>
		</item>
		
		<item>
			<title>Reboot</title>
			<link>https://alexkyllo.com/posts/reboot/</link>
			<pubDate>Mon, 30 Dec 2019 17:16:15 -0800</pubDate>
			
			<guid>https://alexkyllo.com/posts/reboot/</guid>
			<description>I&amp;rsquo;ve read many times that the key to success is just to do things and tell people.
In the new decade, I am going to put much more effort into the tell people part.</description>
			<content type="html"><![CDATA[<p>I&rsquo;ve read many times that the key to success is just to <em><a href="http://carl.flax.ie/dothingstellpeople.html">do things and tell people</a>.</em></p>
<p>In the new decade, I am going to put much more effort into the <em>tell people</em> part.</p>
]]></content>
		</item>
		
	</channel>
</rss>
