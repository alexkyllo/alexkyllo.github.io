<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on Alex Kyllo</title>
		<link>https://alexkyllo.com/posts/</link>
		<description>Recent content in Posts on Alex Kyllo</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>Alex Kyllo 2020</copyright>
		<lastBuildDate>Tue, 22 Dec 2020 21:42:53 -0800</lastBuildDate>
		<atom:link href="https://alexkyllo.com/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Fighting Fakes Fairly</title>
			<link>https://alexkyllo.com/posts/fake-faces/</link>
			<pubDate>Tue, 22 Dec 2020 21:42:53 -0800</pubDate>
			
			<guid>https://alexkyllo.com/posts/fake-faces/</guid>
			<description>I just wrapped up the first quarter of my master&amp;rsquo;s degree program. In my project-driven Machine Learning class, I chose to work on a fake face detection task. I picked this topic mostly because I had no experience with computer vision or convolutional neural networks (CNNs) and wanted to try something totally new to me, plus I knew that there was an obvious ethics component to facial recognition AI and wanted to feature that in my work.</description>
			<content type="html"><![CDATA[<p>I just wrapped up the first quarter of my master&rsquo;s degree program. In
my project-driven Machine Learning class, I chose to work on a fake
face detection task. I picked this topic mostly because I had no
experience with computer vision or convolutional neural networks
(CNNs) and wanted to try something totally new to me, plus I knew that
there was an obvious ethics component to facial recognition AI and
wanted to feature that in my work.</p>
<p>I wasn&rsquo;t expecting the topic to be <em>quite</em> so timely, but after Google
fired Timnit Gebru for questioning both their AI ethics and their
diversity, equality and inclusion practices, I saw <a href="https://twitter.com/Mantzarlis/status/1338220767042002945?s=19">a jaw-dropping
thread</a>
pop up in my Twitter feed. Someone had created a fake account, using
GAN generated fake face profile images, claiming to be one of
Dr. Gebru&rsquo;s former colleagues, in order to smear her reputation.</p>
<p><img src="/alexios-thread.png" alt=""></p>
<p><img src="/jeff-jeffries.jpeg" alt=""></p>
<p><img src="/julia-smith-kleinberg.jpeg" alt=""></p>
<p>Even worse, a professor emeritus from the UW CSE department, Pedro
Domingos, engaged with and even retweeted this account, apparently not
realizing it was a fake:</p>
<p><img src="/julia-smith-kleinberg-thread.jpeg" alt=""></p>
<p>This made it very clear to me that &ldquo;deep fakes&rdquo; is <em>not</em> a toy problem
or a hypothetical future problem, but that the technology is being
deployed <em>right now</em> to spread disinformation online. So this points
to the need for technology to detect and flag fakes, in order to keep
up in the &ldquo;arms race.&rdquo;</p>
<p>My project team took the obvious approach to the problem&ndash;get a
dataset consisting of real human face images and another dataset of
fake ones generated by <a href="https://github.com/NVlabs/stylegan">StyleGAN</a>,
and then train a CNN model to distinguish between the two datasets. We
used Keras, and while it was a bit frustrating to get it configured
and working on top of CUDA, because TensorFlow requires older versions
of the NVIDIA libraries, we were able to get it working.  I hacked
together a little Python framework to run model training with a
specified set of model hyperparameters and image preprocessing steps,
saving the model weights and accuracy results to files to track our
progress. We ended up going with a 10-layer network based on the VGG-16
architecture but simplified to avoid overfitting, because the problem
is much less complex than ImageNet.</p>
<p>While we got promising results (&gt;97% test accuracy) on the original
datasets, we knew that face image datasets tend to have selection bias
toward middle-aged, white faces, so we wanted to test whether our
model performed equally well on subjects of different demographics.</p>
<p>In order to conduct that fairness assessment, we needed a
demographically labeled dataset of both real and fake faces, so we
took the <a href="https://github.com/joojs/fairface">FairFace</a> dataset, then
used
<a href="https://github.com/eladrich/pixel2style2pixel">pixel2style2pixel</a> as
an autoencoder, to embed each image into the latent space of the
StyleGAN network and then extract it back out into an image.</p>
<p>The results of this process were pretty interesting. The
pixel2style2pixel model was able to reconstruct faces very similar to
the originals, removing things like hands partially covering faces and
bruises and blemishes, though it sometimes made mistakes like
misinterpreting head coverings as hair. Here are some samples:</p>
<p><img src="/fair2fake.jpg" alt=""></p>
<p>Here&rsquo;s my own face before and after running through the face falsifier
network, which feels pretty eerie to look at:</p>
<p><img src="/alex-fake.jpg" alt=""></p>
<p>What we found in the process, though, was that our model trained on
the first dataset <strong>totally</strong> failed to generalize to the second
dataset, with accuracy only in the mid 50s, only marginally better
than a coin toss. So we decided not to bother with a fairness
assessment on a clearly useless model.  Instead, we retrained and
scored the model on a combined dataset drawn from both datasets. We
found that this new model performed better than our first model on
both the original dataset and the combined dataset, probably due to
the additional training examples, and it also performed with almost
exactly the same metrics across demographic splits of male/female,
white/non-white, black/non-black, elderly/non-elderly, and
child/non-child.</p>
<p>There&rsquo;s a ton more work to be done in the space, and I still have
doubts about whether the model really detected something intrinsic to
fake faces that will generalize well across more datasets. But for
now, I think this experience demonstrates the value of using a
diverse, heterogenous input dataset, testing the model on truly
out-of-sample data, and considering and planning for a model fairness
assessment up front.</p>
<p>Our code is available on GitHub: <a href="https://github.com/alexkyllo/fake-faces/">https://github.com/alexkyllo/fake-faces/</a></p>
<p>Winter quarter will be a little change of pace as I&rsquo;m taking High
Performance Computing, which will focus on GPU programming with CUDA.
I&rsquo;m pretty excited for it because I hope to brush up on my
computational linear algebra and better understand what is actually
happening when I train a neural network model on my GPU.</p>
]]></content>
		</item>
		
		<item>
			<title>Database Learning Resources</title>
			<link>https://alexkyllo.com/posts/database-learning/</link>
			<pubDate>Sat, 09 May 2020 11:04:29 -0700</pubDate>
			
			<guid>https://alexkyllo.com/posts/database-learning/</guid>
			<description>Over the last few years working in analytics and data science, I&amp;rsquo;ve developed a strong interest in the internal workings of database engines, particularly distributed database for analytical workloads. Here I&amp;rsquo;m going to build a list of computer science concepts and learning resources that I am finding helpful in building understanding of how these systems work and how to implement them.
File I/O Concepts  On Disk I/O a series of blog posts by Alex Petrov, explaining the different methods of file I/O in Linux (buffered, direct, memory-mapped, asynchronous, vectored) and their use cases in the context of database applications, as well as on-disk data structures (SSTables, B+ Trees, Log-Structured Merge Trees)  Probabilistic Data Structures  Bloom Filter A hash bitmap for probabilistic answers to set membership queries (true/false with no false negatives) HyperLogLog A hash based algorithm for fast approximate cardinality (distinct count) estimation Count Min Sketch an approximate frequency table similar to a counting Bloom filter  On-Disk Data Structures  LSM Tree Paper Log Structured Merge Trees&amp;ndash;trees of immutable sorted runs of on-disk data that are periodically merged using external merge sort Google BigTable Paper Application of LSM Trees  Distributed Consensus Algorithms  The Raft Consensus Algorithm Implementing Raft by Eli Bendersky  Comprehensive Database Concept Books  Database System Concepts Database Internals Designing Data Intensive Applications  Open Courses  CMU 15-445 Database Systems covers basics of on-disk database system implementation CMU 15-721 Advanced Database Systems covers advanced topics in in-memory database implementation, with emphasis on concurrency control and optimizations.</description>
			<content type="html"><![CDATA[<p>Over the last few years working in analytics and data science, I&rsquo;ve developed a
strong interest in the internal workings of database engines, particularly
distributed database for analytical workloads. Here I&rsquo;m going to build a list of
computer science concepts and learning resources that I am finding helpful in
building understanding of how these systems work and how to implement them.</p>
<h1 id="file-io-concepts">File I/O Concepts</h1>
<ul>
<li><a href="https://medium.com/databasss/on-disk-io-part-1-flavours-of-io-8e1ace1de017">On Disk I/O</a>
a series of blog posts by Alex Petrov, explaining the different methods of
file I/O in Linux (buffered, direct, memory-mapped, asynchronous, vectored)
and their use cases in the context of database applications, as well as on-disk
data structures (SSTables, B+ Trees, Log-Structured Merge Trees)</li>
</ul>
<h2 id="probabilistic-data-structures">Probabilistic Data Structures</h2>
<ul>
<li><a href="https://www.jasondavies.com/bloomfilter/">Bloom Filter</a> A hash bitmap for
probabilistic answers to set membership queries (true/false with no false negatives)</li>
<li><a href="http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf">HyperLogLog</a> A hash based
algorithm for fast approximate cardinality (distinct count) estimation</li>
<li><a href="https://redislabs.com/blog/count-min-sketch-the-art-and-science-of-estimating-stuff/">Count Min Sketch</a>
an approximate frequency table similar to a counting Bloom filter</li>
</ul>
<h1 id="on-disk-data-structures">On-Disk Data Structures</h1>
<ul>
<li><a href="http://paperhub.s3.amazonaws.com/18e91eb4db2114a06ea614f0384f2784.pdf">LSM Tree Paper</a>
Log Structured Merge Trees&ndash;trees of immutable sorted runs of on-disk data that are periodically
merged using external merge sort</li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Google BigTable Paper</a> Application of LSM Trees</li>
</ul>
<h1 id="distributed-consensus-algorithms">Distributed Consensus Algorithms</h1>
<ul>
<li><a href="https://raft.github.io/">The Raft Consensus Algorithm</a></li>
<li><a href="https://eli.thegreenplace.net/2020/implementing-raft-part-0-introduction/">Implementing Raft by Eli Bendersky</a></li>
</ul>
<h1 id="comprehensive-database-concept-books">Comprehensive Database Concept Books</h1>
<ul>
<li><a href="https://www.db-book.com/db7/index.html">Database System Concepts</a></li>
<li><a href="https://learning.oreilly.com/library/view/database-internals/9781492040330/">Database Internals</a></li>
<li><a href="https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/">Designing Data Intensive Applications</a></li>
</ul>
<h1 id="open-courses">Open Courses</h1>
<ul>
<li><a href="https://15445.courses.cs.cmu.edu/fall2019/">CMU 15-445 Database Systems</a>
covers basics of on-disk database system implementation</li>
<li><a href="https://15721.courses.cs.cmu.edu/spring2020/">CMU 15-721 Advanced Database Systems</a>
covers advanced topics in in-memory database implementation, with emphasis on concurrency
control and optimizations.</li>
</ul>
]]></content>
		</item>
		
		<item>
			<title>Make Is All You Need</title>
			<link>https://alexkyllo.com/posts/make-is-all-you-need/</link>
			<pubDate>Mon, 30 Dec 2019 18:03:52 -0800</pubDate>
			
			<guid>https://alexkyllo.com/posts/make-is-all-you-need/</guid>
			<description>As a data analyst mostly working with high level languages and GUI tools, I thought of GNU Make as a tool (from 1976!) for building Linux binaries from compiled languages, and it didn&amp;rsquo;t occur to me how it could be useful in my workflow. Make is usually part of any curriculum for software engineers learning C/C++, but I seldom if ever see it mentioned in data science courses or tutorials.</description>
			<content type="html"><![CDATA[<p>As a data analyst mostly working with high level languages and GUI tools,
I thought of <a href="https://www.gnu.org/software/make/">GNU Make</a> as a tool
(from 1976!) for building Linux binaries from compiled languages,
and it didn&rsquo;t occur to me how it could be useful in my workflow. Make is
usually part of any curriculum for software engineers learning C/C++, but
I seldom if ever see it mentioned in data science courses or tutorials.</p>
<p>Today, I use Make for all of my software and data science projects, and I
consider it the most important tool for reproducibility. It ships with
Linux and Mac, and there is a <a href="http://gnuwin32.sourceforge.net/packages/make.htm">Windows installer</a>
available. I love that it&rsquo;s language-agnostic, so it&rsquo;s perfect for teams
where different people use R, Python, or other languages for their analysis code.
It also gives you a &ldquo;single point of contact&rdquo; for all the scripts and commands
in your project, so you can just type <code>make</code> or
<code>make [task name]</code> instead of having to remember and re-type each command.</p>
<p>The <a href="https://www.gnu.org/software/make/manual/make.html">GNU Make Manual</a>
is quite approachable while also providing great detail, but in a nutshell,
you create a file called <code>Makefile</code> in your project directory and add to it
a set of &ldquo;rules&rdquo; for producing output files from input files, in this form:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-make" data-lang="make"><span style="color:#a6e22e">targets </span><span style="color:#f92672">:</span> prerequisites
    recipe
</code></pre></div><p>A &ldquo;target&rdquo; and a &ldquo;prerequisite&rdquo; are usually filenames, but can also be just
names for tasks (called &ldquo;phony&rdquo; targets).</p>
<p>So why is this so useful for data science projects?</p>
<p>Data analysis code typically involves five steps for processing data:</p>
<ol>
<li>Getting it</li>
<li>Cleaning it</li>
<li>Exploring it</li>
<li>Modeling it</li>
<li>Interpreting it</li>
</ol>
<p>Without a disciplined approach, a typical R or Python script or notebook file
will do all of these steps together, perhaps interleaved, making it difficult
to pull the steps apart so that they can be executed separately.</p>
<p>Why is it important to separate them? Some steps may process large quantities of
data, so you don&rsquo;t want to re-run everything each time you change your script.
Caching intermediate results saves you a lot of time as you&rsquo;re developing.</p>
<p>Also, if you have mutli-language scenarios, say you want to visualize your data
with <a href="https://ggplot2.tidyverse.org/">ggplot2</a>
before training a predictive model with <a href="https://keras.io/">Keras</a>,
it&rsquo;s easiest to separate these steps into
different scripts in different languages, rather than doing something
complicated like spawning a Python subprocess from within R or vice versa.</p>
<p>Here&rsquo;s an example of what a Makefile might look like, for generating a paper
written in LaTeX, containing two plots generated by an R script and a Python
script, from a CSV file created by a SQL database query:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-make" data-lang="make"><span style="color:#a6e22e">default </span><span style="color:#f92672">:</span> paper.pdf

<span style="color:#a6e22e">paper.pdf </span><span style="color:#f92672">:</span> paper.tex figure-01.png figure-02.png
    pdflatex paper.tex

<span style="color:#a6e22e">figure-01.png </span><span style="color:#f92672">:</span> plot.R data.csv
    Rscript plot.R

<span style="color:#a6e22e">figure-02.png </span><span style="color:#f92672">:</span> plot.py data.csv
    python plot.py

<span style="color:#a6e22e">data.csv</span><span style="color:#f92672">:</span> query.sql
    sql2csv --db <span style="color:#e6db74">&#34;sqlite:///database.db&#34;</span> query.sql
</code></pre></div><p>Given this Makefile, when you type <code>make</code>, it will execute each of these
recipes in dependency order, and then if you type <code>make</code> again, it will
do nothing, because the dependencies haven&rsquo;t changed. Make looks at the
file modified timestamps to see which steps actually need to be redone.
If you change the <code>query.sql</code> file and type <code>make</code> again, it will rerun
everything because that&rsquo;s the first item in the dependency chain, but if you
only change the <code>paper.tex</code> file, then only the
recipe for <code>paper.pdf</code> will be rerun the next time you type <code>make</code>, because
nothing upstream of it changed. This is an incredible time-saver when some of
your recipes take a long time to run, as is often the case with data science projects.
I&rsquo;ve seen many other tools designed to solve this problem, most of them
language-specific and much more complex to learn and use than Make.</p>
<p>As far as how to structure a new project, there isn&rsquo;t really an accepted standard
data science code project structure or framework.
The closest thing might be
<a href="https://drivendata.github.io/cookiecutter-data-science/#directory-structure">Cookiecutter Data Science</a>
(which also utilizes a Makefile), though it&rsquo;s Python-specific and I haven&rsquo;t seen
it used &ldquo;in the wild&rdquo; yet. If you use a Makefile in your project, though, it doesn&rsquo;t
really matter that much how you structure your directories or even what language you use,
as thinking in terms of Make recipes will encourage you to break up your analysis
scripts into a pipeline of small,
dependent, repeatable steps with cached results. Then when a colleague picks up
your project to review your work or enhance it, just looking at the Makefile will
give them a clear idea of how it&rsquo;s put together.
So use Make for your data science projects and encourage your coworkers to do the
same&ndash;you will thank each other!</p>
]]></content>
		</item>
		
		<item>
			<title>Reboot</title>
			<link>https://alexkyllo.com/posts/reboot/</link>
			<pubDate>Mon, 30 Dec 2019 17:16:15 -0800</pubDate>
			
			<guid>https://alexkyllo.com/posts/reboot/</guid>
			<description>I&amp;rsquo;ve read many times that the key to success is just to do things and tell people.
In the new decade, I am going to put much more effort into the tell people part.</description>
			<content type="html"><![CDATA[<p>I&rsquo;ve read many times that the key to success is just to <em><a href="http://carl.flax.ie/dothingstellpeople.html">do things and tell people</a>.</em></p>
<p>In the new decade, I am going to put much more effort into the <em>tell people</em> part.</p>
]]></content>
		</item>
		
	</channel>
</rss>
